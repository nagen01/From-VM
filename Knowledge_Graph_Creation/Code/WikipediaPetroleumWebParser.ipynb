{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EyzxdJYBq-1L"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bOoPB0KAq-1d"
   },
   "source": [
    "### Wiki list of American actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FG9_tviq-1g"
   },
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "BASE_URL = 'https://en.wikipedia.org'\n",
    "MAIN_URL = BASE_URL+ '/wiki/Petroleum'\n",
    "total_added = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3W82I1Jq-1p"
   },
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request(\"GET\", url)\n",
    "    return BeautifulSoup(r.data,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9hl0_F7q-1y"
   },
   "source": [
    "### Text preprocessing - stop word & citation removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXGvUt4Oq-11"
   },
   "outputs": [],
   "source": [
    "# Function returns the negation handled word if it is presend in the appos dictionary\n",
    "# Else returns the word itself\n",
    "def negationHandling(word):\n",
    "    if word in appos:\n",
    "        return appos[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "# Check if a word is a Stopword\n",
    "# Stopword is a word that is commonly present in most of the documents and does not affect the model\n",
    "def isNotStopWord(word):\n",
    "    return word not in stopwords.words('english')\n",
    "\n",
    "\n",
    "def preprocessingText(text):\n",
    "    text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    tokens = []\n",
    "    temp = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #Converting to LowerCase\n",
    "#         words = map(str.lower, words)\n",
    "        \n",
    "        # Remove stop words\n",
    "        words = filter(lambda x: isNotStopWord(x), words)\n",
    "        \n",
    "        # Removing punctuations except '<.>/<?>/<!>'\n",
    "        punctuations = '\"#$%&\\'()*+,-/:;<=>@\\\\^_`{|}~'\n",
    "        words = map(lambda x: x.translate(str.maketrans('', '', punctuations)), words)\n",
    "        \n",
    "        # Remove empty strings\n",
    "        words = filter(lambda x: len(x) > 0, words)\n",
    "      \n",
    "        tokens = tokens + list(words)\n",
    "        temp = ' '.join(word for word in tokens)\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzXyAloZq-2A"
   },
   "source": [
    "### Parsing each linked petroleum webpage and writing into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t--g84vbq-2C"
   },
   "outputs": [],
   "source": [
    "# write the given content into text file with name <title>.txt\n",
    "def write_text_into_file(title, data, modified_data):\n",
    "    os.chdir(r'/home/nb01/C_Drive/Knowledge_Graph_Creation/Dataset/PetroleumDataset')\n",
    "    filename = title + \".txt\"\n",
    "    f = open(filename, 'w+', encoding=\"utf-8\")\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "    \n",
    "    os.chdir(r'/home/nb01/C_Drive/Knowledge_Graph_Creation/Dataset/PetroleumPreprocessedDataset')\n",
    "    filename = title + \".txt\"\n",
    "    f = open(filename, 'w+', encoding=\"utf-8\")\n",
    "    f.write(modified_data)\n",
    "    f.close()\n",
    "    #with open(filename, 'r') as f:\n",
    "    #    print(f)\n",
    "    #    k = f.read()\n",
    "    #    print(k)\n",
    "    print(\"Text file \" + filename + \" created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse each petrolium linked webpage content\n",
    "def parse_webpage_content(link):\n",
    "    soup = get_soup(link)\n",
    "    #results = soup.find_all(\"div\", {\"class\": \"mw-parser-output\"})[0]\n",
    "    no_of_paragraphs = 0\n",
    "    paragraphs = soup.find_all('p')\n",
    "    #print(paragraphs)\n",
    "    data = \"\"\n",
    "    for para in paragraphs:\n",
    "        #if para.id != \"mw-empty-elt\":\n",
    "        data += para.text.strip() +\"\\n\"\n",
    "        #print(data)\n",
    "        #no_of_paragraphs += 1\n",
    "        #if no_of_paragraphs == 3:\n",
    "        #    break\n",
    "            \n",
    "    #extracting 2 sentences from the paragraph\n",
    "    #data = \".\".join(data.split(\".\")[:2])\n",
    "    modified_data = preprocessingText(data+\".\") \n",
    "    return data, modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DZCAoAIq-2O"
   },
   "source": [
    "### Parsing all petroleum linked webpage content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8i6V1Ny8q-2R"
   },
   "outputs": [],
   "source": [
    "# iterate through every group\n",
    "def parse_all_petroleums_from_wiki(url):\n",
    "    #name = 'petroleum'\n",
    "    #data = parse_webpage_content(url) #Getting main page data\n",
    "    #write_text_into_file(name, data) #Writting main page data\n",
    "    \n",
    "    #Processing the Pages that link to \"Petroleum\"\n",
    "    soup = get_soup(url)\n",
    "    #Getting link for petroleum linked url\n",
    "    linked_url = soup.find_all(\"div\", {\"class\":\"portal\", \"id\":\"p-tb\"})[0].find_next('ul').find_all('li')[0].a['href'].strip()\n",
    "    #print(BASE_URL + linked_url)\n",
    "    soup = get_soup(BASE_URL + linked_url)\n",
    "    #print(soup)\n",
    "    results = soup.find_all(\"div\", {\"id\":\"mw-content-text\"})\n",
    "    #print(results)\n",
    "    no_of_petroleum_links = 0\n",
    "    for res in results:\n",
    "        # iterator through every actor or <li> element\n",
    "        li_list = res.find_next('ul').find_all('li')\n",
    "        #print(li_list)\n",
    "        for li in li_list:\n",
    "            name = li.a.text.strip()\n",
    "            #print(name)\n",
    "            link = li.a['href'].strip()\n",
    "            #print(link)\n",
    "            data, modified_data = parse_webpage_content(BASE_URL+link)\n",
    "            write_text_into_file(name, data, modified_data)\n",
    "            no_of_petroleum_links += 1\n",
    "            print(no_of_petroleum_links)\n",
    "            if no_of_petroleum_links == 2:\n",
    "                break\n",
    "    print(no_of_petroleum_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKDuHfxnq-2b"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parse_all_petroleums_from_wiki(MAIN_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXfVmaZbq-2t",
    "outputId": "b1566aee-73c1-4286-dade-966cc548caec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file Alaska.txt created\n",
      "1\n",
      "Text file Alkane.txt created\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WikipediaActorsWebParser.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
