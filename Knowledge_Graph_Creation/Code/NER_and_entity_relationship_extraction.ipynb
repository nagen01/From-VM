{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UY_2vuloN5gG"
   },
   "source": [
    "### Named Entity Recognition and Entity Relationship extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55815,
     "status": "ok",
     "timestamp": 1556934637068,
     "user": {
      "displayName": "Vidhyalakshimi Sreenivasan",
      "photoUrl": "https://lh4.googleusercontent.com/-USDjX2HH3qk/AAAAAAAAAAI/AAAAAAAAAAc/AhuvJIOSFK0/s64/photo.jpg",
      "userId": "08716189862880610765"
     },
     "user_tz": 240
    },
    "id": "WbPXWizTiAOh",
    "outputId": "51499702-d39c-4915-f712-cee22885599d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aKLdosZriJgo"
   },
   "outputs": [],
   "source": [
    "# Copy data to google colab from google drive and unzip\n",
    "# This may take 1-2 minutes\n",
    "!cp gdrive/My\\ Drive/ADBI\\ Project/ADBI_Capstone_Project/Dataset/ActorsPreprocessedDataset/*.txt .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1112
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15032,
     "status": "ok",
     "timestamp": 1556935028615,
     "user": {
      "displayName": "Vidhyalakshimi Sreenivasan",
      "photoUrl": "https://lh4.googleusercontent.com/-USDjX2HH3qk/AAAAAAAAAAI/AAAAAAAAAAc/AhuvJIOSFK0/s64/photo.jpg",
      "userId": "08716189862880610765"
     },
     "user_tz": 240
    },
    "id": "wXs1fNswgrHV",
    "outputId": "2cfb884b-7b1b-4d5f-ca3e-3119830b0fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/fb/323c81288ab9b0b9cc955dcebfd04773d7982307c1a6d559b5a30000825b/textacy-0.6.3-py2.py3-none-any.whl (145kB)\n",
      "\u001b[K     |████████████████████████████████| 153kB 5.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.9.0.1)\n",
      "Collecting srsly>=0.0.5 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/97/47753e3393aa4b18de9f942fac26f18879d1ae950243a556888f389d1398/srsly-0.0.5-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
      "\u001b[K     |████████████████████████████████| 184kB 45.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.2.1)\n",
      "Collecting python-levenshtein>=0.12.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 19.8MB/s \n",
      "\u001b[?25hCollecting ijson>=2.3 (from textacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/e9/8508c5f4987ba238a2b169e582c1f70a47272b22a2f1fb06b9318201bb9e/ijson-2.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.6/dist-packages (from textacy) (4.28.1)\n",
      "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.20.3)\n",
      "Collecting ftfy<5.0.0,>=4.2.0 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 19.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.0.18)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (1.16.3)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (2.21.0)\n",
      "Collecting mwparserfromhell>=0.4.4 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/a2/5d537c16e761db25592b2e69c0071aee475f200ef231ee5ca1a499ae54bc/mwparserfromhell-0.5.3.tar.gz (132kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 44.6MB/s \n",
      "\u001b[?25hCollecting pyphen>=0.9.4 (from textacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 46.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (3.1.0)\n",
      "Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from textacy) (0.5.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz>=0.8.0->textacy) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein>=0.12.0->textacy) (41.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.11->textacy) (4.4.0)\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy) (0.1.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (1.0.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2.0.1)\n",
      "Requirement already satisfied: regex==2018.01.10 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2018.1.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (2.0.2)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.2.9)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (6.12.1)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (0.9.6)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0->textacy) (1.35)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->textacy) (2019.3.9)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (1.12.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy) (0.5.1)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (0.5.6)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (1.10.11)\n",
      "Requirement already satisfied: msgpack-numpy<0.4.4 in /usr/local/lib/python3.6/dist-packages (from thinc<6.13.0,>=6.12.1->spacy>=2.0.0->textacy) (0.4.3.2)\n",
      "Building wheels for collected packages: python-levenshtein, ftfy, mwparserfromhell\n",
      "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
      "  Building wheel for mwparserfromhell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/68/e9/75/f7fcd23cc94c77fc6d07793abcabda7c8829042c177f47028b\n",
      "Successfully built python-levenshtein ftfy mwparserfromhell\n",
      "Installing collected packages: srsly, python-levenshtein, ijson, ftfy, mwparserfromhell, pyphen, textacy\n",
      "Successfully installed ftfy-4.4.3 ijson-2.3 mwparserfromhell-0.5.3 pyphen-0.9.5 python-levenshtein-0.12.0 srsly-0.0.5 textacy-0.6.3\n"
     ]
    }
   ],
   "source": [
    "# ! pip install spacy\n",
    "! pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jb570ZfaZMTk"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "from textacy.extract import subject_verb_object_triples\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N8NxxoZDZMTx"
   },
   "outputs": [],
   "source": [
    "# def url_to_string(url):\n",
    "#     res = requests.get(url)\n",
    "#     html = res.text\n",
    "#     soup = BeautifulSoup(html, 'html5lib')\n",
    "#     for script in soup([\"script\", \"style\", 'aside']):\n",
    "#         script.extract()\n",
    "#     return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n",
    "# text = url_to_string('https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&action=click&pgtype=Homepage&clickSource=story-heading&module=first-column-region&region=top-news&WT.nav=top-news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r1KuNnvnZMT-"
   },
   "source": [
    "# Knowledge Graph generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3Y_xJ9OZMUB"
   },
   "outputs": [],
   "source": [
    "def generate_graphviz_graph(entity_relations, name, verbose=True):\n",
    "    \"\"\"digraph G {\n",
    "    # a -> b [ label=\"a to b\" ];\n",
    "    # b -> c [ label=\"another label\"];\n",
    "    }\"\"\"\n",
    "    graph = list()\n",
    "    graph.append('digraph {')\n",
    "    for er in entity_relations:\n",
    "        graph.append('\"{}\" -> \"{}\" [ label=\"{}\" ];'.format(er[0], er[2], er[1]))\n",
    "    graph.append('}')\n",
    "\n",
    "    out_dot = name + '.dot'\n",
    "    with open(out_dot, 'w') as output_file:\n",
    "        output_file.writelines(graph)\n",
    "\n",
    "    out_png = name + '.png'\n",
    "    DOT_BIN_PATH = 'dot'\n",
    "    command = \"dot -Tpng {} -o {}\".format(out_dot, out_png)\n",
    " \n",
    "    os.system(command)\n",
    "\n",
    "    print('Wrote graph to {} and {}'.format(out_dot, out_png))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7abTZnxYZMUP"
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/priyankha/Documents/ADBI/Capstone/ActorsPreprocessedDataset/'\n",
    "TEXTS = [open(data_dir+f).read() for f in os.listdir(data_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKuwhTJeZMVH"
   },
   "outputs": [],
   "source": [
    "#text = \"Leonardo da Vinci or simply Leonardo, was an Italian polymath of the Renaissance whose areas of interest included invention, drawing, painting, sculpting, architecture, science, music, mathematics, engineering, literature, anatomy, geology, astronomy, botany, writing, history, and cartography. He has been variously called the father of palaeontology, ichnology, and architecture, and he is widely considered one of the greatest painters of all time. Sometimes credited with the inventions of the parachute, helicopter, and tank, he epitomised the Renaissance humanist ideal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2E70INrwZMVU"
   },
   "source": [
    "# Subject Verb Object Generation using Spacy's subject_verb_object_triples method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KYKvs3HZMVe",
    "outputId": "2199a8b9-a661-4644-db63-4090e9b4ac6e"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-26599fa41ae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msvo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentity_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "final_svos = []\n",
    "final_text_svos = []\n",
    "entity_dict = {}\n",
    "svo_labels = []\n",
    "for i, text in enumerate(TEXTS):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent not in entity_dict.keys():\n",
    "            entity_dict[str(ent)] = ent.label_       \n",
    "    svos = list(subject_verb_object_triples(doc))\n",
    "    svos_text = [(str(x[0]).strip(), str(x[1]).strip(), str(x[2]).strip()) for x in svos]\n",
    "    final_svos = final_svos + svos\n",
    "    final_text_svos = final_text_svos + svos_text\n",
    "\n",
    "for svo in final_text_svos:\n",
    "    tup = (None, None)\n",
    "    if(svo[0] in entity_dict.keys()):\n",
    "        tup[0] = entity_dict[svo[0]]\n",
    "    \n",
    "    if(svo[2] in entity_dict.keys()):\n",
    "        tup[1] = entity_dict[svo[2]]\n",
    "    svo_labels.append(tup)\n",
    "    \n",
    "    #generate_graphviz_graph(svos,str(i))\n",
    "#spans = list(doc.ents) + list(doc.noun_chunks)\n",
    "#for span in spans:\n",
    "#    span.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VToFo5ysZMV1",
    "outputId": "a840018f-e8ed-4495-c51d-ec2933be0615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(final_svos[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2PqIuyhqZMWB",
    "outputId": "f6661d45-6e4d-4c84-a542-5c24e6d2afcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g1sTdi-RZMWN"
   },
   "source": [
    "# Subject Verb Object using Spacy's Github code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eshav_y2ZMWQ"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "SUBJECTS = [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"]\n",
    "OBJECTS = [\"dobj\", \"dative\", \"attr\", \"oprd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49gAYvaIZMWb"
   },
   "outputs": [],
   "source": [
    "def getSubsFromConjunctions(subs):\n",
    "    moreSubs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreSubs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreSubs) > 0:\n",
    "                moreSubs.extend(getSubsFromConjunctions(moreSubs))\n",
    "    return moreSubs\n",
    "\n",
    "def getObjsFromConjunctions(objs):\n",
    "    moreObjs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreObjs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(moreObjs) > 0:\n",
    "                moreObjs.extend(getObjsFromConjunctions(moreObjs))\n",
    "    return moreObjs\n",
    "\n",
    "def getVerbsFromConjunctions(verbs):\n",
    "    moreVerbs = []\n",
    "    for verb in verbs:\n",
    "        rightDeps = {tok.lower_ for tok in verb.rights}\n",
    "        if \"and\" in rightDeps:\n",
    "            moreVerbs.extend([tok for tok in verb.rights if tok.pos_ == \"VERB\"])\n",
    "            if len(moreVerbs) > 0:\n",
    "                moreVerbs.extend(getVerbsFromConjunctions(moreVerbs))\n",
    "    return moreVerbs\n",
    "\n",
    "def findSubs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verbNegated = isNegated(head)\n",
    "            subs.extend(getSubsFromConjunctions(subs))\n",
    "            return subs, verbNegated\n",
    "        elif head.head != head:\n",
    "            return findSubs(head)\n",
    "    elif head.pos_ == \"NOUN\":\n",
    "        return [head], isNegated(tok)\n",
    "    return [], False\n",
    "\n",
    "def isNegated(tok):\n",
    "    negations = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "    for dep in list(tok.lefts) + list(tok.rights):\n",
    "        if dep.lower_ in negations:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def findSVs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "def getObjsFromPrepositions(deps):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"ADP\" and dep.dep_ == \"prep\":\n",
    "            objs.extend([tok for tok in dep.rights if tok.dep_  in OBJECTS or (tok.pos_ == \"PRON\" and tok.lower_ == \"me\")])\n",
    "    return objs\n",
    "\n",
    "def getObjsFromAttrs(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(getObjsFromPrepositions(rights))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getObjFromXComp(deps):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(getObjsFromPrepositions(rights))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "def getAllSubs(v):\n",
    "    verbNegated = isNegated(v)\n",
    "    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(getSubsFromConjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verbNegated = findSubs(v)\n",
    "        subs.extend(foundSubs)\n",
    "    return subs, verbNegated\n",
    "\n",
    "def getAllObjs(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "    objs.extend(getObjsFromPrepositions(rights))\n",
    "\n",
    "    #potentialNewVerb, potentialNewObjs = getObjsFromAttrs(rights)\n",
    "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "    #    objs.extend(potentialNewObjs)\n",
    "    #    v = potentialNewVerb\n",
    "\n",
    "    potentialNewVerb, potentialNewObjs = getObjFromXComp(rights)\n",
    "    if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "        objs.extend(potentialNewObjs)\n",
    "        v = potentialNewVerb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(getObjsFromConjunctions(objs))\n",
    "    return v, objs\n",
    "\n",
    "def findSVOs(tokens):\n",
    "    svos = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\" and tok.dep_ != \"aux\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = getAllSubs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            v, objs = getAllObjs(v)\n",
    "            for sub in subs:\n",
    "                for obj in objs:\n",
    "                    objNegated = isNegated(obj)\n",
    "                    svos.append((sub.lower_, \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, obj.lower_))\n",
    "    return svos\n",
    "def printDeps(toks):\n",
    "    for tok in toks:\n",
    "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HW1_vr8gZMWj",
    "outputId": "9adb1563-e4ce-434f-d1c8-5fbf77bcd6b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vinci', 'was', 'polymath'), ('areas', 'included', 'invention'), ('he', 'called', 'father'), ('he', 'considered', 'one'), ('he', 'epitomised', 'ideal')]\n"
     ]
    }
   ],
   "source": [
    "svos = findSVOs(doc)\n",
    "print(svos)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NER_and_entity_relationship_extraction.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
